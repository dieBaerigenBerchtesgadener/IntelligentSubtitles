{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import string\n",
    "import re\n",
    "from types import SimpleNamespace\n",
    "from collections import defaultdict\n",
    "\n",
    "# Scientific computing and ML imports \n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset, WeightedRandomSampler\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.metrics import (\n",
    "    f1_score, \n",
    "    classification_report,\n",
    "    balanced_accuracy_score,\n",
    "    roc_auc_score,\n",
    "    recall_score, \n",
    "    fbeta_score\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.stats import entropy\n",
    "\n",
    "# NLP related imports\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM, pipeline\n",
    "from faster_whisper import WhisperModel\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import nltk\n",
    "from cefrpy import CEFRAnalyzer\n",
    "import contractions\n",
    "from fast_langdetect import detect\n",
    "from simalign import SentenceAligner\n",
    "from typing import Optional\n",
    "\n",
    "# Data handling and visualization\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import wandb\n",
    "\n",
    "# Install GPU acceleration package if needed\n",
    "# %pip install flash-attn  # Uncomment to install GPU acceleration\n",
    "\n",
    "# Download required NLTK data\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "# Set global device (GPU if available, otherwise CPU)\n",
    "global device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Define Name & Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "NAME = \"\"\n",
    "AUDIO_TYPE = \".mp4\"\n",
    "\n",
    "# Paths to audio and reference text files\n",
    "AUDIO_FILE = NAME + AUDIO_TYPE\n",
    "REFERENCE_FILE = f\"{NAME}.srt\"\n",
    "FILTERED_SRT = f\"{NAME}[Filtered].srt\"\n",
    "EXCLUDED_WORDS = [\n",
    "    \"mm\", \"nah\", \"uh\", \"um\", \"whoa\", \"uhm\", \"uhmm\", \"ah\", \"mm-hmm\", \n",
    "    \"uh-huh\", \"uh-uh\", \"uh-uhm\", \"uh-uhmm\", \"uhm-h\", \"aw\", \"ugh\", \n",
    "    \"shh\", \"mmhmm\", \"huh\", \"hmm\", \"mmm\", \"oops\", \"oopsie\", \"uh-oh\", \n",
    "    \"whoops\", \"oof\", \"yup\", \"yep\", \"nope\", \"aha\", \"tsk\", \"ew\", \"phew\", \n",
    "    \"meh\", \"huh-uh\", \"huh-huh\", \"huh-uhm\", \"mhm\", \"oh\", \"hmm-m\", \n",
    "    \"er\", \"eh\", \"ahh\", \"yikes\", \"yawn\", \"ugh-ugh\", \"jeez\", \"duh\", \n",
    "    \"wow\", \"meh-meh\", \"uhhh\", \"ummm\", \"ugh-huh\", \"hmpf\", \"yawn-yawn\", \n",
    "    \"heh\", \"hmph\", \"eep\", \"gah\", \"uhp\", \"boo\", \"psst\", \"argh\", \"oi\", \n",
    "    \"ohh\", \"oh-ho\", \"whoa-whoa\", \"la\", \"laa\", \"ah-ha\", \"ha\", \"ha-ha\", \n",
    "    \"hahaha\", \"bah\", \"whew\", \"ehh\", \"huff\", \"uff\", \"sniff\", \"snort\", \n",
    "    \"gulp\", \"hic\", \"haah\", \"bleh\", \"blah\", \"bla\", \"mwaa\", \"uhuh\", \n",
    "    \"yah\", \"uhw\", \"eww\", \"ewww\", \"grr\", \"huh-huh\", \"haha\", \"shush\", \n",
    "    \"wha\", \"wham\", \"bam\", \"oooh\", \"aaah\", \"hrr\", \"uhhhhhh\", \"ummmmm\", \n",
    "    \"woah\", \"ughugh\", \"mm-mm\", \"uh-huh-huh\", \"erm\", \"grrr\", \"urr\", \n",
    "    \"yippie\", \"oops-a-daisy\", \"ouch\", \"eek\", \"zoinks\", \"woopsie\", \n",
    "    \"yeesh\", \"hm-mm\", \"uhhuh\", \"hrrmph\", \"bleugh\", \"rawr\", \"ick\", \n",
    "    \"whaa\", \"la-la\", \"meep\", \"pfft\", \"haaa\", \"ahhhhhh\", \"oii\", \"tsk-tsk\", \n",
    "    \"blub\", \"blurgh\", \"brr\", \"rrr\", \"oomph\", \"ohhhhhh\", \"hmmmmmm\", \n",
    "    \"ahhhhhhh\", \"guh\", \"ack\", \"zzzz\", \"hush\", \"hsh\", \"boo-hoo\", \"ho-hum\", \n",
    "    \"urrgh\", \"grumble\", \"murmur\", \"mutter\", \"uhhhhmm\", \"hah\", \"ah-ah\", \n",
    "    \"shoo\", \"la-la-la\", \"blah-blah\", \"tra-la\", \"lalala\", \"waah\", \"waaah\", \n",
    "    \"ooh-ooh\", \"uhh\", \"uhhhh\", \"erhm\", \"ermm\", \"urrggh\", \"aargh\", \n",
    "    \"hm-mm-mm\", \"uh-uh-uh\", \"uhm-uh\", \"hurmph\", \"grmph\", \"ha-umph\", \n",
    "    \"um-hum\", \"humph\", \"shhhhhh\", \"psssh\", \"whisper\", \"moan\", \"groan\", \n",
    "    \"ah-choo\", \"cough\", \"sneeze\", \"hick\", \"hiccup\", \"snore\", \"whaaat\", \n",
    "    \"doh\", \"hmh\", \"pfft-pfft\", \"chatter\", \"rumble\", \"buzz\", \"mumble\", \n",
    "    \"ooh-la-la\", \"ahem\", \"tut\", \"hrrmm\", \"grmph\", \"sigh\", \"gulp-gulp\", \n",
    "    \"oh-wow\", \"yeehaw\", \"oh-no\", \"ach\", \"achoo\", \"whoop\", \"zipp\", \"zzz\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Create DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bracketless_lines(original_srt_file: str) -> list[str]:\n",
    "    \"\"\"\n",
    "    Reads the original SRT file line by line and removes all content in brackets\n",
    "    (...), [...], {...} only in text lines.\n",
    "    Timestamps, line numbers, empty lines etc. remain unchanged.\n",
    "    Returns the modified lines as a list.\n",
    "    \"\"\"\n",
    "    bracketless_lines = []\n",
    "    with open(original_srt_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            # Only modify text lines (not timestamps, numbers or empty lines)\n",
    "            if '-->' not in line and not line.strip().isdigit() and line.strip():\n",
    "                line = re.sub(r'\\[.*?\\]|\\(.*?\\)|\\{.*?\\}', '', line)\n",
    "            bracketless_lines.append(line)\n",
    "    return bracketless_lines\n",
    "\n",
    "\n",
    "def remove_brackets_in_text(line: str) -> str:\n",
    "    \"\"\"\n",
    "    Removes text in (), [] and {} from a text line.\n",
    "    \"\"\"\n",
    "    return re.sub(r'\\[.*?\\]|\\(.*?\\)|\\{.*?\\}', '', line)\n",
    "\n",
    "\n",
    "def clean_token(token: str) -> Optional[str]:\n",
    "    \"\"\"\n",
    "    Cleans individual words:\n",
    "    - Remove leading/trailing punctuation\n",
    "    - Convert to lowercase\n",
    "    - Empty strings -> None\n",
    "    \"\"\"\n",
    "    token = token.strip(string.punctuation + '\"\"\\'').lower()\n",
    "    return token if token else None\n",
    "\n",
    "\n",
    "def read_srt_in_memory(srt_path: str) -> list[str]:\n",
    "    \"\"\"\n",
    "    Reads the original SRT file into a list of lines.\n",
    "    Removes bracket content ONLY in lines that are not timestamps\n",
    "    and not pure number lines. Empty lines or time lines\n",
    "    remain unchanged.\n",
    "    \"\"\"\n",
    "    lines = []\n",
    "    with open(srt_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            if '-->' not in line and not line.strip().isdigit() and line.strip():\n",
    "                # This is a \"text\" line -> remove brackets\n",
    "                line = remove_brackets_in_text(line)\n",
    "            lines.append(line)\n",
    "    return lines\n",
    "\n",
    "\n",
    "def custom_split_into_sentences(text: str) -> list[str]:\n",
    "    \"\"\"\n",
    "    First searches for pairs of musical notes (♪), so that everything between\n",
    "    two ♪ (inclusive) becomes ONE sentence segment, e.g. \"♪ Yeah, baby ♪\".\n",
    "    Everything that doesn't fall into such a pair is processed normally.\n",
    "    Then each segment is further split via sent_tokenize if additional\n",
    "    sentence boundaries exist.\n",
    "    \"\"\"\n",
    "    # First split by ♪, but keep them in the list (capture group)\n",
    "    raw_parts = re.split(r'(♪)', text)\n",
    "\n",
    "    merged_segments = []\n",
    "    i = 0\n",
    "    while i < len(raw_parts):\n",
    "        current = raw_parts[i].strip()\n",
    "\n",
    "        # Check if this element is a musical note and if\n",
    "        # raw_parts[i+2] contains another ♪\n",
    "        if current == '♪' and (i + 2) < len(raw_parts) and raw_parts[i+2].strip() == '♪':\n",
    "            # Example: [\"♪\", \" Yeah, baby \", \"♪\"]\n",
    "            inner_text = raw_parts[i+1].strip() if (i+1 < len(raw_parts)) else \"\"\n",
    "            merged_segments.append(f'♪ {inner_text} ♪')\n",
    "            i += 3  # Skip the three used elements\n",
    "        else:\n",
    "            # If no ♪-pair exists, just take the current element (ignore empty strings)\n",
    "            if current:\n",
    "                merged_segments.append(current)\n",
    "            i += 1\n",
    "\n",
    "    # Now merged_segments contains either \"♪ Yeah, baby ♪\"\n",
    "    # or \"some text without musical notes\" or both mixed.\n",
    "    # Then we split each segment using sent_tokenize just in case\n",
    "    # (e.g. if there's a period or question mark in \"♪ ...\").\n",
    "    final_sentences = []\n",
    "    for segment in merged_segments:\n",
    "        # segment could be \"♪ Yeah, baby ♪\" OR \"This is a sentence. And another one.\"\n",
    "        for s in sent_tokenize(segment):\n",
    "            s = s.strip()\n",
    "            if s:\n",
    "                final_sentences.append(s)\n",
    "\n",
    "    return final_sentences\n",
    "\n",
    "\n",
    "def extract_tokens_with_sentences(srt_lines: list[str]) -> list[dict]:\n",
    "    \"\"\"\n",
    "    Extracts tokens and their corresponding original sentences from the SRT lines,\n",
    "    correctly handling sentences that span multiple lines. Additionally, text\n",
    "    between musical note pairs is wrapped in its own sentence \"♪ ... ♪\".\n",
    "    \"\"\"\n",
    "    tokens_with_sentences = []\n",
    "    full_text = \"\"\n",
    "\n",
    "    # Combine all text lines into a single string\n",
    "    for line in srt_lines:\n",
    "        if '-->' not in line and not line.strip().isdigit() and line.strip():\n",
    "            cleaned_line = line.strip().lstrip('-').strip()\n",
    "            full_text += cleaned_line + \" \"\n",
    "\n",
    "    # Segment at musical note pairs and sentence boundaries\n",
    "    sentences = custom_split_into_sentences(full_text)\n",
    "\n",
    "    # Extract tokens from each sentence and store the original sentence\n",
    "    for sentence in sentences:\n",
    "        words = sentence.split()\n",
    "        position = 0\n",
    "        for w in words:\n",
    "            position += 1\n",
    "            t = clean_token(w)\n",
    "            if t:\n",
    "                tokens_with_sentences.append({\n",
    "                    'token': t,\n",
    "                    'original_sentence': sentence,\n",
    "                    'position': position\n",
    "                })\n",
    "\n",
    "    return tokens_with_sentences\n",
    "\n",
    "\n",
    "# Read and process the SRT file\n",
    "srt_lines_in_memory = read_srt_in_memory(REFERENCE_FILE)\n",
    "original_tokens = extract_tokens_with_sentences(srt_lines_in_memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Audio Complexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Whisper model\n",
    "AudioModel = WhisperModel(\n",
    "    \"tiny\",\n",
    "    device=\"cpu\",\n",
    "    compute_type=\"int8\",\n",
    "    cpu_threads=1,\n",
    "    num_workers=1\n",
    ")\n",
    "\n",
    "# Transcribe audio with word timestamps and probabilities\n",
    "segments, info = AudioModel.transcribe(\n",
    "    AUDIO_FILE,\n",
    "    word_timestamps=True,\n",
    "    beam_size=1\n",
    ")\n",
    "\n",
    "def clean_word(token):\n",
    "    \"\"\"Clean word by removing punctuation and standardizing apostrophes\"\"\"\n",
    "    return token.strip(string.punctuation + '\"\"'').replace(\" \", \"\").replace(\"'\", \"'\").lower()\n",
    "\n",
    "def remove_apostrophes(word):\n",
    "    \"\"\"Remove apostrophes for comparison\"\"\"\n",
    "    return re.sub(r\"['']\", \"\", word).lower()\n",
    "\n",
    "# Extract predicted tokens with probabilities and timestamps\n",
    "predicted_tokens = []\n",
    "for segment in segments:\n",
    "    for word_info in segment.words:\n",
    "        cleaned = clean_word(word_info.word)\n",
    "        probability = word_info.probability\n",
    "        timestep = word_info.start\n",
    "        predicted_tokens.append({\n",
    "            'token': cleaned,\n",
    "            'probability': probability,\n",
    "            'timestep': timestep\n",
    "        })\n",
    "\n",
    "original_sequence = [t['token'] for t in original_tokens]\n",
    "predicted_sequence = [t['token'] for t in predicted_tokens]\n",
    "\n",
    "# Perform sequence alignment using difflib.SequenceMatcher\n",
    "matcher = difflib.SequenceMatcher(None, original_sequence, predicted_sequence)\n",
    "aligned_results = []\n",
    "\n",
    "for opcode in matcher.get_opcodes():\n",
    "    tag, i1, i2, j1, j2 = opcode\n",
    "    \n",
    "    if tag == 'equal':\n",
    "        # Direct 1:1 mapping between original and prediction\n",
    "        for idx_orig, idx_pred in zip(range(i1, i2), range(j1, j2)):\n",
    "            aligned_results.append({\n",
    "                'word': original_tokens[idx_orig]['token'],\n",
    "                'audio_complexity': 1 - predicted_tokens[idx_pred]['probability'],\n",
    "                'timestep': predicted_tokens[idx_pred]['timestep']\n",
    "            })\n",
    "\n",
    "    elif tag == 'replace':\n",
    "        # Handle different cases of replacements\n",
    "        orig_joined = \" \".join(original_sequence[i1:i2]).lower()\n",
    "        pred_joined = \" \".join(predicted_sequence[j1:j2]).lower()\n",
    "\n",
    "        # Case 1: Handle contractions (e.g., \"he's\" ↔ \"he is\")\n",
    "        if len(predicted_sequence[j1:j2]) == 1 and contractions.fix(predicted_sequence[j1]) == orig_joined:\n",
    "            pred = predicted_tokens[j1]\n",
    "            for idx_orig in range(i1, i2):\n",
    "                aligned_results.append({\n",
    "                    'word': original_tokens[idx_orig]['token'],\n",
    "                    'audio_complexity': 1 - pred['probability'],\n",
    "                    'timestep': pred['timestep']\n",
    "                })\n",
    "\n",
    "        # Case 2: Compare without apostrophes (e.g., \"name's\" ↔ \"names\")\n",
    "        elif remove_apostrophes(orig_joined) == remove_apostrophes(pred_joined):\n",
    "            pred = predicted_tokens[j1]\n",
    "            for idx_orig in range(i1, i2):\n",
    "                aligned_results.append({\n",
    "                    'word': original_tokens[idx_orig]['token'],\n",
    "                    'audio_complexity': 1 - pred['probability'],\n",
    "                    'timestep': pred['timestep']\n",
    "                })\n",
    "\n",
    "        # Fallback: Handle non-matching words\n",
    "        else:\n",
    "            for idx_orig in range(i1, i2):\n",
    "                aligned_results.append({\n",
    "                    'word': original_tokens[idx_orig]['token'],\n",
    "                    'audio_complexity': 1.0,\n",
    "                    'timestep': None\n",
    "                })\n",
    "\n",
    "    elif tag == 'delete':\n",
    "        # Handle words present in original but missing in prediction\n",
    "        for idx_orig in range(i1, i2):\n",
    "            aligned_results.append({\n",
    "                'word': original_tokens[idx_orig]['token'],\n",
    "                'audio_complexity': 1.0,\n",
    "                'timestep': None\n",
    "            })\n",
    "\n",
    "    elif tag == 'insert':\n",
    "        # Skip additional words in prediction\n",
    "        pass\n",
    "\n",
    "# Create DataFrame from results\n",
    "df = pd.DataFrame(aligned_results)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Filter in/out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_tokens_non_english(original_tokens, exception_words):\n",
    "    \"\"\"\n",
    "    1) Detects language of each sentence once and caches it in sentence_language_cache\n",
    "    2) If sentence != 'en' and != 'la', each token is checked individually:\n",
    "       - If token detected as non 'en'/'la' -> process=True, display=True, set_manually=True\n",
    "       - Otherwise -> process=False, display=False, set_manually=False  \n",
    "    3) If sentence is English/Latin, all flags are set to False\n",
    "    \n",
    "    Returns DataFrame with same number of rows as original_tokens (index-synchronized)\n",
    "    \"\"\"\n",
    "    # Step 1: Detect language once per sentence (cache)\n",
    "    unique_sentences = {item['original_sentence'] for item in original_tokens}\n",
    "    sentence_language_cache = {}\n",
    "    for sentence in unique_sentences:\n",
    "        cleaned_sentence = sentence.replace(\"\\n\", \"\").strip()\n",
    "        if cleaned_sentence:\n",
    "            result = detect(cleaned_sentence, low_memory=False)\n",
    "            sentence_language_cache[sentence] = result[\"lang\"]\n",
    "        else:\n",
    "            sentence_language_cache[sentence] = None  # No language detectable\n",
    "\n",
    "    # Step 2: Check if sentence != 'en'/'la'; if yes, check token individually\n",
    "    token_info = []\n",
    "    for item in original_tokens:\n",
    "        sent_lang = sentence_language_cache.get(item[\"original_sentence\"], None)\n",
    "        \n",
    "        # If sentence detected as English or Latin -> all False\n",
    "        if sent_lang in (\"en\", \"la\"):\n",
    "            token_info.append({\n",
    "                \"word\": item[\"token\"],\n",
    "                \"display\": False,\n",
    "                \"set_manually\": False\n",
    "            })\n",
    "        else:\n",
    "            # Sentence not en/la -> check token individually\n",
    "            token_text = item[\"token\"].strip()\n",
    "            if token_text:\n",
    "                if token_text in exception_words:\n",
    "                    # Word in exception list -> all False\n",
    "                    token_info.append({\n",
    "                        \"word\": item[\"token\"],\n",
    "                        \"display\": False,\n",
    "                        \"set_manually\": False\n",
    "                    })\n",
    "                else:\n",
    "                    word_detection = detect(token_text, low_memory=False)\n",
    "                    word_lang = word_detection[\"lang\"]\n",
    "                    if word_lang in (\"en\", \"la\"):\n",
    "                        # Word is English/Latin -> all False\n",
    "                        token_info.append({\n",
    "                            \"word\": item[\"token\"],\n",
    "                            \"display\": False,\n",
    "                            \"set_manually\": False\n",
    "                        })\n",
    "                    else:\n",
    "                        # Word not English or Latin\n",
    "                        token_info.append({\n",
    "                            \"word\": item[\"token\"],\n",
    "                            \"display\": True,\n",
    "                            \"set_manually\": True\n",
    "                        })\n",
    "                        print(f\"Token '{item['token']}' in sentence '{item['original_sentence']}' is not English/Latin.\")\n",
    "            else:\n",
    "                # Empty token -> all False optional\n",
    "                token_info.append({\n",
    "                    \"word\": item[\"token\"],\n",
    "                    \"display\": False,\n",
    "                    \"set_manually\": False\n",
    "                })\n",
    "\n",
    "    return pd.DataFrame(token_info)\n",
    "\n",
    "def mark_non_english_in_df(df: pd.DataFrame, original_tokens: list[dict], exception_words = [\"i\", \"no\"]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Creates helper DataFrame (df_language) using detect_tokens_non_english()\n",
    "    and transfers process, display, set_manually columns to df,\n",
    "    synchronized with original_tokens index.\n",
    "    \"\"\"\n",
    "    # Create DataFrame with columns based on word-by-word check\n",
    "    df_language = detect_tokens_non_english(original_tokens, exception_words)\n",
    "\n",
    "    # Transfer columns to df, assuming df and original_tokens are index-synchronized \n",
    "    df[\"display\"] = df_language[\"display\"]\n",
    "    df[\"set_manually\"] = df_language[\"set_manually\"]\n",
    "\n",
    "    return df\n",
    "\n",
    "def mark_notes_in_df(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Mark musical notes and text between note symbols in DataFrame\"\"\"\n",
    "    skip = False\n",
    "    marked_words = []  \n",
    "    \n",
    "    # Process all rows\n",
    "    for idx, row in df.iterrows():\n",
    "        if row[\"word\"] == \"♪\":\n",
    "            df.at[idx, 'display'] = True\n",
    "            df.at[idx, 'set_manually'] = True\n",
    "            df.at[idx, 'process'] = False\n",
    "            marked_words.append(row[\"word\"])\n",
    "            skip = not skip\n",
    "            continue\n",
    "        \n",
    "        if skip:\n",
    "            df.at[idx, 'display'] = True\n",
    "            df.at[idx, 'set_manually'] = True\n",
    "            df.at[idx, 'process'] = False\n",
    "            marked_words.append(row[\"word\"]) \n",
    "    \n",
    "    # Print summary of marked words\n",
    "    if marked_words:\n",
    "        print(\"\\nIncluded words:\")\n",
    "        print(\", \".join(marked_words))\n",
    "    \n",
    "    return df\n",
    "\n",
    "def mark_excluded_words(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Mark words from EXCLUDED_WORDS list in DataFrame\"\"\"\n",
    "    mask = df['word'].isin(EXCLUDED_WORDS)\n",
    "    df.loc[mask, ['display', 'set_manually', 'process']] = [False, True, True]\n",
    "    \n",
    "    total_excluded = mask.sum()\n",
    "    print(f\"\\nExcluded {total_excluded} words based on EXCLUDED_WORDS list\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "def mark_numbers_in_df(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Mark numbers greater than 13 in DataFrame\"\"\"\n",
    "    marked_words = []\n",
    "    for idx, row in df.iterrows():\n",
    "        try:\n",
    "            num = float(row['word'])\n",
    "            if num > 13:\n",
    "                df.at[idx, 'display'] = True\n",
    "                df.at[idx, 'set_manually'] = True\n",
    "                df.at[idx, 'process'] = True\n",
    "                marked_words.append(row['word'])\n",
    "        except ValueError:\n",
    "            continue\n",
    "            \n",
    "    if marked_words:\n",
    "        print(\"\\nIncluded numbers:\")\n",
    "        print(\", \".join(marked_words))\n",
    "    return df\n",
    "\n",
    "# Initialize columns\n",
    "df['display'] = None\n",
    "df['set_manually'] = False\n",
    "df['process'] = True\n",
    "\n",
    "# Process DataFrame\n",
    "exception_words = [\"i\", \"no\", \"so\"]\n",
    "df = mark_non_english_in_df(df, original_tokens, exception_words)\n",
    "df = mark_notes_in_df(df)\n",
    "df = mark_excluded_words(df)\n",
    "df = mark_numbers_in_df(df)\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(df.to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translation_model_name = \"Helsinki-NLP/opus-mt-en-de\"\n",
    "device = 0 if torch.cuda.is_available() else -1\n",
    "translator_pipeline = pipeline(\n",
    "    \"translation\",\n",
    "    model=translation_model_name,\n",
    "    device=device,\n",
    "    max_length=512,\n",
    "    early_stopping=True\n",
    ")\n",
    "\n",
    "aligner = SentenceAligner(\n",
    "    model=\"bert\",\n",
    "    token_type=\"bpe\",\n",
    "    matching_methods=\"mai\",\n",
    "    device='cuda' if torch.cuda.is_available() else 'cpu'\n",
    ")\n",
    "\n",
    "def batch_translate_and_align(all_tokens, batch_size=32):\n",
    "    SPECIAL_TOKENS = {\n",
    "        \"♪\": \"♪\"\n",
    "    }\n",
    "\n",
    "    # Remember the original index for restoring order later\n",
    "    for i, tok in enumerate(all_tokens):\n",
    "        tok[\"original_index\"] = i\n",
    "\n",
    "    # Group tokens by their original sentence\n",
    "    sentence_to_tokens = defaultdict(list)\n",
    "    for token_data in all_tokens:\n",
    "        sentence_en = token_data[\"original_sentence\"]\n",
    "        sentence_to_tokens[sentence_en].append(token_data)\n",
    "\n",
    "    unique_sentences = list(sentence_to_tokens.keys())\n",
    "\n",
    "    # Translate unique sentences in batches\n",
    "    translations = []\n",
    "    for i in range(0, len(unique_sentences), batch_size):\n",
    "        batch_sentences = unique_sentences[i : i + batch_size]\n",
    "        batch_translations = translator_pipeline(batch_sentences)\n",
    "        translations.extend(batch_translations)\n",
    "\n",
    "    # Map each English sentence to its German translation\n",
    "    sentence_to_de = {}\n",
    "    for i, sentence_en in enumerate(unique_sentences):\n",
    "        sentence_to_de[sentence_en] = translations[i][\"translation_text\"]\n",
    "\n",
    "    # Align each sentence once\n",
    "    alignment_info = {}\n",
    "    for sentence_en in unique_sentences:\n",
    "        # Sort tokens by position for alignment (but they still remember their original_index)\n",
    "        token_list = sorted(sentence_to_tokens[sentence_en], key=lambda x: x[\"position\"])\n",
    "        src_tokens = [td[\"token\"] for td in token_list]\n",
    "\n",
    "        german_sent = sentence_to_de[sentence_en]\n",
    "        tgt_tokens = german_sent.split()\n",
    "\n",
    "        aligns = aligner.get_word_aligns(src_tokens, tgt_tokens)\n",
    "        alignment_pairs = aligns[\"inter\"]\n",
    "\n",
    "        alignment_info[sentence_en] = {\n",
    "            \"german_sentence\": german_sent,\n",
    "            \"src_tokens\": src_tokens,\n",
    "            \"tgt_tokens\": tgt_tokens,\n",
    "            \"alignment_pairs\": alignment_pairs\n",
    "        }\n",
    "\n",
    "    # Build final results for each token\n",
    "    results = []\n",
    "    for sentence_en, token_data_list in sentence_to_tokens.items():\n",
    "        info = alignment_info[sentence_en]\n",
    "        german_sent = info[\"german_sentence\"]\n",
    "        tgt_tokens = info[\"tgt_tokens\"]\n",
    "        alignment_pairs = info[\"alignment_pairs\"]\n",
    "\n",
    "        # Sort by position for alignment\n",
    "        token_data_list_sorted = sorted(token_data_list, key=lambda x: x[\"position\"])\n",
    "        for idx, token_data in enumerate(token_data_list_sorted):\n",
    "            token = token_data[\"token\"]\n",
    "            if token in SPECIAL_TOKENS:\n",
    "                aligned_words_cleaned = SPECIAL_TOKENS[token]\n",
    "            else:\n",
    "                # Find all target indices in the alignment for this source index (idx)\n",
    "                aligned_indices = [\n",
    "                    tgt_idx for (src_idx, tgt_idx) in alignment_pairs\n",
    "                    if src_idx == idx\n",
    "                ]\n",
    "                if not aligned_indices:\n",
    "                    # If no aligned word, do single-word translation\n",
    "                    single_word_translation = translator_pipeline([token])[0][\"translation_text\"]\n",
    "                    aligned_words_cleaned = single_word_translation\n",
    "                else:\n",
    "                    # Join all aligned words in the target\n",
    "                    aligned_words = [tgt_tokens[t_i] for t_i in aligned_indices if 0 <= t_i < len(tgt_tokens)]\n",
    "                    aligned_words_cleaned = \" \".join(\n",
    "                        w.translate(str.maketrans('', '', string.punctuation))\n",
    "                        for w in aligned_words\n",
    "                    )\n",
    "\n",
    "            results.append({\n",
    "                \"original_index\": token_data[\"original_index\"],\n",
    "                \"english_token\": token,\n",
    "                \"english_sentence\": sentence_en,\n",
    "                \"german_translation\": aligned_words_cleaned,\n",
    "                \"german_full_sentence\": german_sent\n",
    "            })\n",
    "\n",
    "    # Important: restore original order\n",
    "    results_sorted = sorted(results, key=lambda x: x[\"original_index\"])\n",
    "    return results_sorted\n",
    "\n",
    "# Suppose you already have original_tokens as in your code\n",
    "result = batch_translate_and_align(original_tokens, batch_size=1)\n",
    "\n",
    "# Now create your DataFrame in the same order as original_tokens\n",
    "df_translations = pd.DataFrame(result)\n",
    "df['translation'] = df_translations['german_translation']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(df.to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Word Occurrence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary to keep track of word counts and calculate the normalized values directly\n",
    "word_counts = {}\n",
    "\n",
    "# Function to get count and update dictionary with normalized value\n",
    "def get_normalized_occurrence(word, max=15): # More than 15 per word is unnecessary and more detailed values for smaller values\n",
    "    word_counts[word] = word_counts.get(word, 0) + 1\n",
    "    # Normalize between 0 and 1 using min=1 and max=200\n",
    "    if word_counts[word] >= max:\n",
    "        return 1.0\n",
    "    return (word_counts[word] - 1) / (max - 1)\n",
    "\n",
    "# Add column showing normalized occurrence value only for rows where process is True\n",
    "df.loc[df['process'], 'word_occurrence'] = df.loc[df['process'], 'word'].apply(get_normalized_occurrence)\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Word Complexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Analyzer class\n",
    "analyzer = CEFRAnalyzer()\n",
    "\n",
    "# CEFR levels mapping\n",
    "cefr_levels = {\n",
    "    'A1': 0.0,\n",
    "    'A2': 0.2,\n",
    "    'B1': 0.4,\n",
    "    'B2': 0.6,\n",
    "    'C1': 0.8,\n",
    "    'C2': 1.0\n",
    "}\n",
    "\n",
    "# Function to get complexity score for a word\n",
    "def get_word_complexity(word):\n",
    "    try:\n",
    "        level = analyzer.get_average_word_level_CEFR(word)\n",
    "        if level is not None:\n",
    "            score = cefr_levels.get(level.name, 0.0)\n",
    "            #print(f\"Word: {word}, Level: {level.name}, Score: {score}\")\n",
    "            return score\n",
    "    except Exception as e:\n",
    "        print(f\"Error retrieving level for word '{word}': {e}\")\n",
    "        return 0.0\n",
    "    return 0.0\n",
    "\n",
    "# Compute word complexities\n",
    "df.loc[df['process'], 'word_complexity'] = df.loc[df['process'], 'word'].apply(get_word_complexity)\n",
    "\n",
    "# Output updated DataFrame\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Sentence Complexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_sentence_complexities(original_tokens):\n",
    "    \"\"\"\n",
    "    Calculate sentence complexities and map them to individual words.\n",
    "    Returns DataFrame with complexity scores normalized by maximum entropy.\n",
    "    \"\"\"\n",
    "    def calculate_entropy(sentence):\n",
    "        \"\"\"Calculate entropy score for a given sentence using MLM predictions\"\"\"\n",
    "        inputs = tokenizer_mlm(sentence, return_tensors='pt')\n",
    "        with torch.no_grad():\n",
    "            outputs = model_mlm(**inputs)\n",
    "        predictions = torch.softmax(outputs.logits, dim=-1)\n",
    "        \n",
    "        token_probs = []\n",
    "        for i, token_id in enumerate(inputs.input_ids[0]):\n",
    "            token_prob = predictions[0, i, token_id].item()\n",
    "            token_probs.append(token_prob)\n",
    "        \n",
    "        return entropy(token_probs)\n",
    "\n",
    "    # Calculate max_entropy using a complex reference sentence\n",
    "    reference_sentence = \"I used to believe that technology could save us from the climate crisis, that all the big brains in the world would come up with a silver bullet to stop carbon pollution, that a clever policy would help that technology spread, and our concern about the greenhouse gases heating the planet would be a thing of the past, and we wouldn't have to worry about the polar bears anymore.\"\n",
    "    max_entropy = calculate_entropy(reference_sentence)\n",
    "\n",
    "    # Create cache for computed sentence complexities\n",
    "    sentence_complexity_cache = {}\n",
    "    \n",
    "    # Calculate complexity only for unique sentences\n",
    "    unique_sentences = set(item['original_sentence'] for item in original_tokens)\n",
    "    for sentence in unique_sentences:\n",
    "        entropy_value = calculate_entropy(sentence)\n",
    "        normalized_entropy = entropy_value / max_entropy\n",
    "        sentence_complexity_cache[sentence] = normalized_entropy\n",
    "    \n",
    "    # Create result list using cached values\n",
    "    word_entries = [\n",
    "        {\n",
    "            'word': item['token'],\n",
    "            'sentence_complexity': sentence_complexity_cache[item['original_sentence']]\n",
    "        }\n",
    "        for item in original_tokens\n",
    "    ]\n",
    "    \n",
    "    return pd.DataFrame(word_entries)\n",
    "\n",
    "# Initialize tokenizer and model for Masked Language Model\n",
    "model_id = \"answerdotai/ModernBERT-base\"\n",
    "tokenizer_mlm = AutoTokenizer.from_pretrained(model_id)\n",
    "model_mlm = AutoModelForMaskedLM.from_pretrained(model_id)\n",
    "\n",
    "# Calculate sentence complexities and add to DataFrame\n",
    "df_sentence_complexity = calculate_sentence_complexities(original_tokens)\n",
    "df[\"sentence_complexity\"] = df_sentence_complexity[\"sentence_complexity\"]\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Word Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_word_importance_single(sentence, original_token, position, tokenizer_mlm, model_mlm):\n",
    "    \"\"\"\n",
    "    Compute importance of a word in a sentence.\n",
    "    \n",
    "    Args:\n",
    "        sentence: Original sentence (string)\n",
    "        original_token: Original token (e.g. \"he's\")\n",
    "        position: 1-based position of token in sentence (e.g. 1 = first word)\n",
    "        tokenizer_mlm: Hugging Face tokenizer for Masked LM\n",
    "        model_mlm: Hugging Face model for Masked LM\n",
    "    \n",
    "    Returns:\n",
    "        float: Importance score between 0 and 1\n",
    "    \"\"\"\n",
    "    # Tokenize entire sentence with word IDs\n",
    "    encoding = tokenizer_mlm(\n",
    "        sentence,\n",
    "        return_tensors='pt',\n",
    "        add_special_tokens=True\n",
    "    )\n",
    "        \n",
    "    # word_ids() returns index of original word for each sub-token\n",
    "    # or None (e.g. for [CLS], [SEP])\n",
    "    word_ids = encoding.word_ids(batch_index=0)  # List of length seq_len\n",
    "\n",
    "    # Target is (position - 1) since word_ids() starts at 0\n",
    "    # while 'position' in original_tokens starts at 1\n",
    "    target_word_id = position - 1\n",
    "    \n",
    "    # Find all sub-token indices belonging to this word\n",
    "    subtoken_indices = [i for i, w_id in enumerate(word_ids) if w_id == target_word_id]\n",
    "    \n",
    "    # If no sub-tokens found, word may not exist or position mismatch -> importance = 0\n",
    "    if not subtoken_indices:\n",
    "        return 0.0\n",
    "    \n",
    "    # Create copy of token IDs for masking\n",
    "    masked_input = {k: v.clone() for k, v in encoding.items() if isinstance(v, torch.Tensor)}\n",
    "    \n",
    "    # Replace all relevant subtokens with [MASK]\n",
    "    for idx in subtoken_indices:\n",
    "        masked_input['input_ids'][0, idx] = tokenizer_mlm.mask_token_id\n",
    "    \n",
    "    # Run inference\n",
    "    with torch.no_grad():\n",
    "        outputs = model_mlm(**masked_input)\n",
    "    predictions = outputs.logits  # [batch_size, seq_len, vocab_size]\n",
    "    \n",
    "    # Calculate probability of original token\n",
    "    # Use product of probabilities for all subtokens (common choice)\n",
    "    prob_product = 1.0\n",
    "    \n",
    "    for idx in subtoken_indices:\n",
    "        # Softmax for current subtoken\n",
    "        softmax_probs = torch.softmax(predictions[0, idx], dim=-1)\n",
    "        original_token_id = encoding.input_ids[0, idx]  # ID of \"real\" subtoken\n",
    "        subtoken_prob = softmax_probs[original_token_id].item()\n",
    "        prob_product *= subtoken_prob\n",
    "    \n",
    "    # Define importance as (1 - product of subtoken probabilities)\n",
    "    importance = 1.0 - prob_product\n",
    "    return importance\n",
    "\n",
    "# Calculate word importance for each token\n",
    "word_importances = []\n",
    "for i, item in enumerate(original_tokens):\n",
    "    # Check if processing is requested for this row according to DataFrame\n",
    "    if df.loc[i, \"process\"]:\n",
    "        importance = compute_word_importance_single(\n",
    "            sentence=item[\"original_sentence\"],\n",
    "            original_token=item[\"token\"],\n",
    "            position=item[\"position\"],\n",
    "            tokenizer_mlm=tokenizer_mlm,\n",
    "            model_mlm=model_mlm\n",
    "        )\n",
    "    else:\n",
    "        # Set default value if process=False\n",
    "        importance = None\n",
    "    \n",
    "    word_importances.append(importance)\n",
    "\n",
    "# Add importances to DataFrame and format\n",
    "df[\"word_importance\"] = word_importances\n",
    "df[\"word_importance\"] = df[\"word_importance\"].apply(lambda x: f\"{x:.10f}\")\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_names = ['dataset1.csv', 'datatset2.csv', 'dataset3.csv']\n",
    "\n",
    "dfs = []\n",
    "for file_name in file_names:\n",
    "    df_temp = pd.read_csv(file_name)\n",
    "    dfs.append(df_temp)\n",
    "\n",
    "df = pd.concat(dfs, ignore_index=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Random NN as Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data\n",
    "X = df[['audio_complexity', 'word_complexity', 'sentence_complexity', \n",
    "        'word_importance', 'word_occurrence']].values\n",
    "y = df['display'].astype(int).values  # Convert to integer\n",
    "\n",
    "# Calculate baseline random accuracy\n",
    "random_accuracy = 1 - df['display'].mean()\n",
    "print(f\"Random Accuracy: {random_accuracy * 100:.2f}%\")\n",
    "\n",
    "# Split into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Evaluate baseline models\n",
    "# 1. Stratified random baseline\n",
    "dummy_clf = DummyClassifier(strategy='stratified', random_state=42)\n",
    "dummy_clf.fit(X, y)\n",
    "y_pred = dummy_clf.predict(X)\n",
    "y_pred_proba = dummy_clf.predict_proba(X)\n",
    "\n",
    "# 2. Most frequent class baseline\n",
    "dummy_clf_freq = DummyClassifier(strategy='most_frequent', random_state=42)\n",
    "dummy_clf_freq.fit(X, y)\n",
    "y_pred_freq = dummy_clf_freq.predict(X)\n",
    "\n",
    "# Calculate metrics\n",
    "print(\"\\nBaseline Performance Metrics:\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# F1 Scores\n",
    "baseline_f1 = f1_score(y, y_pred)\n",
    "baseline_f1_freq = f1_score(y, y_pred_freq)\n",
    "print(f\"F1 Score (stratified): {baseline_f1:.4f}\")\n",
    "print(f\"F1 Score (most frequent): {baseline_f1_freq:.4f}\")\n",
    "\n",
    "# F2 Scores\n",
    "f2_score_stratified = fbeta_score(y, y_pred, beta=2)\n",
    "f2_score_most_frequent = fbeta_score(y, y_pred_freq, beta=2)\n",
    "print(f\"F2 Score (stratified): {f2_score_stratified:.4f}\")\n",
    "print(f\"F2 Score (most frequent): {f2_score_most_frequent:.4f}\")\n",
    "\n",
    "# Additional metrics\n",
    "bal_acc = balanced_accuracy_score(y, y_pred)\n",
    "roc_auc = roc_auc_score(y, y_pred_proba[:, 1])\n",
    "print(f\"Balanced Accuracy: {bal_acc:.4f}\")\n",
    "print(f\"ROC-AUC: {roc_auc:.4f}\")\n",
    "\n",
    "# Class distribution\n",
    "print(f\"\\nClass distribution:\")\n",
    "print(np.bincount(y))\n",
    "\n",
    "# Detailed classification reports\n",
    "print(\"\\nClassification Report (stratified):\")\n",
    "print(classification_report(y, y_pred))\n",
    "\n",
    "print(\"\\nClassification Report (most frequent):\")\n",
    "print(classification_report(y, y_pred_freq))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BinaryClassifier(nn.Module):\n",
    "    def __init__(self, input_features):\n",
    "        super(BinaryClassifier, self).__init__()\n",
    "        \n",
    "        # Create dynamic layer list\n",
    "        layers = []\n",
    "        current_size = input_features\n",
    "        \n",
    "        # Build hidden layers dynamically\n",
    "        for hidden_size in wandb.config.hidden_layers:\n",
    "            layers.extend([\n",
    "                nn.Linear(current_size, hidden_size),\n",
    "                nn.BatchNorm1d(hidden_size),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(wandb.config.dropout_rate)\n",
    "            ])\n",
    "            current_size = hidden_size\n",
    "        \n",
    "        # Add final output layer\n",
    "        layers.append(nn.Linear(current_size, 1))\n",
    "        \n",
    "        # Create sequential model\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# Custom Loss Functions\n",
    "class FocalLoss(nn.Module):\n",
    "    \"\"\"Focal loss for binary classification\"\"\"\n",
    "    def __init__(self, alpha=1, gamma=2):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "    \n",
    "    def forward(self, inputs, targets):\n",
    "        # Ensure tensors have same shape\n",
    "        inputs = inputs.view(-1)\n",
    "        targets = targets.view(-1)\n",
    "        \n",
    "        bce_loss = F.binary_cross_entropy(inputs, targets, reduction='none')\n",
    "        pt = torch.exp(-bce_loss)\n",
    "        focal_loss = self.alpha * (1-pt)**self.gamma * bce_loss\n",
    "        return torch.mean(focal_loss)\n",
    "\n",
    "class FocalLossWithSigmoid(nn.Module):\n",
    "    def __init__(self, alpha=1, gamma=2):\n",
    "        super().__init__()\n",
    "        self.focal = FocalLoss(alpha=alpha, gamma=gamma)\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        inputs = torch.sigmoid(inputs)   # Sigmoid here\n",
    "        return self.focal(inputs, targets)\n",
    "\n",
    "class AsymmetricLoss(nn.Module):\n",
    "    def __init__(self, gamma_neg=4, gamma_pos=1, clip=0.05):\n",
    "        super(AsymmetricLoss, self).__init__()\n",
    "        self.gamma_neg = gamma_neg\n",
    "        self.gamma_pos = gamma_pos\n",
    "        self.clip = clip\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        targets = targets.view(-1, 1)\n",
    "        inputs = torch.clamp(inputs, self.clip, 1 - self.clip)\n",
    "        \n",
    "        # Positive samples\n",
    "        pt_pos = torch.where(targets == 1, inputs, torch.ones_like(inputs))\n",
    "        loss_pos = -torch.log(pt_pos) * torch.pow(1 - pt_pos, self.gamma_pos) * targets\n",
    "        \n",
    "        # Negative samples\n",
    "        pt_neg = torch.where(targets == 0, 1 - inputs, torch.ones_like(inputs))\n",
    "        loss_neg = -torch.log(pt_neg) * torch.pow(1 - pt_neg, self.gamma_neg) * (1 - targets)\n",
    "        \n",
    "        return torch.mean(loss_pos + loss_neg)\n",
    "\n",
    "class BCEWithSigmoid(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.bce = nn.BCELoss()\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        inputs = torch.sigmoid(inputs)   # Sigmoid here\n",
    "        return self.bce(inputs, targets)\n",
    "\n",
    "class AsymmetricLossWithSigmoid(nn.Module):\n",
    "    def __init__(self, gamma_neg=4, gamma_pos=1, clip=0.05):\n",
    "        super().__init__()\n",
    "        self.asl = AsymmetricLoss(gamma_neg=gamma_neg, gamma_pos=gamma_pos, clip=clip)\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        inputs = torch.sigmoid(inputs)   # Sigmoid here\n",
    "        return self.asl(inputs, targets)\n",
    "\n",
    "class LLoss(nn.Module):\n",
    "    def __init__(self, beta=1):\n",
    "        super(LLoss, self).__init__()\n",
    "        self.beta = beta\n",
    "    \n",
    "    def forward(self, inputs, targets):\n",
    "        targets = targets.view(-1, 1)\n",
    "        L = 1 + torch.pow(inputs - targets, 2)\n",
    "        L = torch.log(L)\n",
    "        return torch.mean(L)\n",
    "\n",
    "class MLoss(nn.Module):\n",
    "    def __init__(self, beta=1):\n",
    "        super(MLoss, self).__init__()\n",
    "        self.beta = beta\n",
    "    \n",
    "    def forward(self, inputs, targets):\n",
    "        targets = targets.view(-1, 1)\n",
    "        M = torch.abs(inputs - targets)\n",
    "        M = 1 - torch.exp(-self.beta * M)\n",
    "        return torch.mean(M)\n",
    "\n",
    "# Loss Function Factory\n",
    "def get_loss_function(loss_name):\n",
    "    if loss_name == \"BCE\":\n",
    "        return BCEWithSigmoid().to(device)\n",
    "    elif loss_name == \"BCEWithLogits\":\n",
    "        pos_weight = torch.tensor([(y_train == 0).sum() / (y_train == 1).sum()]).to(device)\n",
    "        return nn.BCEWithLogitsLoss(pos_weight=pos_weight).to(device)\n",
    "    elif loss_name == \"Focal\":\n",
    "        return FocalLossWithSigmoid(\n",
    "            alpha=wandb.config.loss_params[\"focal_alpha\"],\n",
    "            gamma=wandb.config.loss_params[\"focal_gamma\"]\n",
    "        ).to(device)\n",
    "    elif loss_name == \"Asymmetric\":\n",
    "        return AsymmetricLossWithSigmoid(\n",
    "            gamma_neg=wandb.config.loss_params[\"asymmetric_gamma_neg\"],\n",
    "            gamma_pos=wandb.config.loss_params[\"asymmetric_gamma_pos\"],\n",
    "            clip=wandb.config.loss_params[\"asymmetric_clip\"]\n",
    "        ).to(device)\n",
    "    elif loss_name == \"L\":\n",
    "        return LLoss(beta=wandb.config.loss_params[\"l_beta\"]).to(device)\n",
    "    elif loss_name == \"M\":\n",
    "        return MLoss(beta=wandb.config.loss_params[\"m_beta\"]).to(device)\n",
    "    else:\n",
    "        raise ValueError(f\"Unbekannte Loss-Funktion: {loss_name}\")\n",
    "\n",
    "def get_optimizer(model, optimizer_name):\n",
    "    if optimizer_name == \"AdamW\":\n",
    "        return torch.optim.AdamW(\n",
    "            model.parameters(),\n",
    "            lr=wandb.config.learning_rate,\n",
    "            weight_decay=wandb.config.weight_decay\n",
    "        )\n",
    "    elif optimizer_name == \"Adam\":\n",
    "        return torch.optim.Adam(\n",
    "            model.parameters(),\n",
    "            lr=wandb.config.learning_rate,\n",
    "            weight_decay=wandb.config.weight_decay\n",
    "        )\n",
    "    elif optimizer_name == \"RMSprop\":\n",
    "        return torch.optim.RMSprop(\n",
    "            model.parameters(),\n",
    "            lr=wandb.config.learning_rate,\n",
    "            weight_decay=wandb.config.weight_decay\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(f\"Optimizer {optimizer_name} nicht unterstützt\")\n",
    "\n",
    "def train_epoch(model, train_loader, criterion, optimizer):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    predictions = []\n",
    "    true_labels = []\n",
    "    \n",
    "    for X_batch, y_batch in train_loader:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_batch)\n",
    "        loss = criterion(outputs.squeeze(), y_batch)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        predictions.extend((torch.sigmoid(outputs) > 0.5).squeeze().cpu().detach().numpy())\n",
    "        true_labels.extend(y_batch.cpu().numpy())\n",
    "    \n",
    "    # Berechne beide Metriken\n",
    "    f1 = f1_score(true_labels, predictions)\n",
    "    f2 = fbeta_score(true_labels, predictions, beta=2.0)\n",
    "    \n",
    "    return total_loss / len(train_loader), {\"f1\": f1, \"f2\": f2}\n",
    "\n",
    "def calculate_recall_pos(all_targets, all_preds):\n",
    "    return recall_score(all_targets, all_preds, pos_label=1)\n",
    "\n",
    "def evaluate(model, val_loader, criterion=None, is_final=False, is_test=False):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "    all_probs = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in val_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            \n",
    "            outputs = model(X_batch)\n",
    "            if not is_final:\n",
    "                loss = criterion(outputs.squeeze(), y_batch)\n",
    "                total_loss += loss.item()\n",
    "            \n",
    "            probs = torch.sigmoid(outputs)\n",
    "            preds = (probs > 0.5).float()\n",
    "            \n",
    "            all_preds.extend(preds.view(-1).cpu().numpy())\n",
    "            all_targets.extend(y_batch.cpu().numpy())\n",
    "            all_probs.extend(probs.view(-1).cpu().numpy())\n",
    "    \n",
    "    f1 = f1_score(all_targets, all_preds)\n",
    "    f2 = fbeta_score(all_targets, all_preds, beta=2.0) \n",
    "    \n",
    "    if is_final:\n",
    "        bacc = balanced_accuracy_score(all_targets, all_preds)\n",
    "        auc = roc_auc_score(all_targets, all_probs)\n",
    "        \n",
    "        print('\\nFinale Evaluierung:')\n",
    "        print(f'F1-Score: {f1:.4f}')\n",
    "        print(f'F2-Score: {f2:.4f}')  \n",
    "        print(f'Balanced Accuracy: {bacc:.4f}')\n",
    "        print(f'ROC-AUC: {auc:.4f}')\n",
    "        \n",
    "        if not is_test:\n",
    "            wandb.log({\n",
    "                \"final_f1\": f1,\n",
    "                \"final_f2\": f2, \n",
    "                \"final_balanced_accuracy\": bacc,\n",
    "                \"final_roc_auc\": auc\n",
    "            })\n",
    "        \n",
    "        return f1, f2, bacc, auc  \n",
    "    else:\n",
    "        avg_loss = total_loss / len(val_loader)\n",
    "        return avg_loss, {\"f1\": f1, \"f2\": f2}  \n",
    "\n",
    "\n",
    "def train_model(model, train_loader, val_loader, epochs=None, patience=None):    \n",
    "    epochs = epochs if epochs is not None else wandb.config.epochs\n",
    "    patience = patience if patience is not None else wandb.config.early_stopping_patience\n",
    "    \n",
    "    criterion = get_loss_function(wandb.config.loss_function)\n",
    "    optimizer = get_optimizer(model, wandb.config.optimizer)\n",
    "    \n",
    "    best_val_f2 = 0  \n",
    "    patience_counter = 0\n",
    "    \n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    train_f1s = []\n",
    "    train_f2s = []\n",
    "    val_f1s = []\n",
    "    val_f2s = []  \n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        train_loss, train_metrics = train_epoch(model, train_loader, criterion, optimizer)\n",
    "        val_loss, val_metrics = evaluate(model, val_loader, criterion, is_final=False)\n",
    "        \n",
    "        val_f1 = val_metrics[\"f1\"]\n",
    "        val_f2 = val_metrics[\"f2\"]  \n",
    "        \n",
    "        print(f'Epoch {epoch:03d} | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f} | Val F1: {val_f1:.4f} | Val F2: {val_f2:.4f}')\n",
    "        \n",
    "        wandb.log({\n",
    "            'epoch': epoch,\n",
    "            'train_loss': train_loss,\n",
    "            'val_loss': val_loss,\n",
    "            'val_f1': val_f1,\n",
    "            'val_f2': val_f2 \n",
    "        })\n",
    "        \n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "        train_f1s.append(train_metrics[\"f1\"])\n",
    "        train_f2s.append(train_metrics[\"f2\"])\n",
    "        val_f1s.append(val_f1)\n",
    "        val_f2s.append(val_f2)  \n",
    "        \n",
    "        # Modell-Speicherung basierend auf F2-Score\n",
    "        if val_f2 > best_val_f2:  \n",
    "            best_val_f2 = val_f2\n",
    "            torch.save(model.state_dict(), 'best_model.pth')\n",
    "            wandb.save('best_model.pth')\n",
    "            print(f'Epoch {epoch:03d}: New best model saved with F2 = {val_f2:.4f}')\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "\n",
    "        \n",
    "        if patience_counter >= patience:\n",
    "            print(f'Early stopping at epoch {epoch}')\n",
    "            break\n",
    "    \n",
    "    # Load best model\n",
    "    model.load_state_dict(torch.load('best_model.pth'))\n",
    "    \n",
    "    # Plot training curves\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(train_losses, label='Train Loss')\n",
    "    plt.plot(val_losses, label='Val Loss')\n",
    "    plt.title('Loss Curves')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(train_f2s, label='Train FBeta')\n",
    "    plt.plot(val_f2s, label='Val FBeta')\n",
    "    plt.title('Recall Scores')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Recall Score')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Final evaluation\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    true_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in val_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device) # Daten auf GPU laden\n",
    "            outputs = model(X_batch)\n",
    "            predictions.extend((torch.sigmoid(outputs) > 0.5).squeeze().cpu().numpy())\n",
    "            true_labels.extend(y_batch.cpu().numpy())\n",
    "    \n",
    "    print(\"\\nFinal Classification Report:\")\n",
    "    print(classification_report(true_labels, predictions))\n",
    "    \n",
    "    return model\n",
    "\n",
    "def prepare_data():\n",
    "    # Daten vorbereiten\n",
    "    X = df[['audio_complexity', 'word_complexity', 'sentence_complexity', 'word_importance', 'word_occurrence']].values\n",
    "    y = df['display'].astype(int).values\n",
    "\n",
    "    global y_train, X_train\n",
    "    # Aufteilen in Training und Validation\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y) # stratify für Klassenbalance wie im Original\n",
    "\n",
    "    # Umwandeln in Tensoren, für Pytorch notwendig\n",
    "    X_train = torch.FloatTensor(X_train).to(device) # Tensoren direkt auf GPU laden\n",
    "    X_val = torch.FloatTensor(X_val).to(device) # Tensoren direkt auf GPU laden\n",
    "    y_train = torch.FloatTensor(y_train).to(device) # Tensoren direkt auf GPU laden\n",
    "    y_val = torch.FloatTensor(y_val).to(device) # Tensoren direkt auf GPU laden\n",
    "\n",
    "    train_dataset = TensorDataset(X_train, y_train) # TensorDataset, simpler als Dataset\n",
    "    val_dataset = TensorDataset(X_val, y_val)\n",
    "\n",
    "    global train_loader\n",
    "    # Gewichtetes Sampling für unbalancierte Klassen\n",
    "    if wandb.config.weighted_sampler:\n",
    "        class_counts = np.bincount(y_train.int().cpu().numpy()) # .int() für Datentype int32, .numpy() für Numpy-Array, bincount für Klassenverteilung (bei [0, 1, 1, 2, 2, 2] = [1, 2, 3])\n",
    "        # [3771  317]\n",
    "        weights = 1.0 / torch.tensor(class_counts, dtype=torch.float) # Klassen werden je nach Anzahl in Trainingsdaten gewichtet\n",
    "        # 1 / tensor([3771.,  317.]) > tensor([0.0003, 0.0032])\n",
    "        samples_weights = weights[y_train.int().cpu()] # Gewichtung für jedes Sample, also false * 0.0003 und true * 0.0032 > damit hat true höhere P gezogen zu werden\n",
    "\n",
    "        sampler = WeightedRandomSampler(\n",
    "            weights=samples_weights,\n",
    "            num_samples=len(samples_weights),\n",
    "            replacement=True # Mit replacement=True können die wenigen Samples der Minderheitsklasse mehrfach verwendet werden, bei false wäre gewichtetes Sampling nicht möglich\n",
    "        )\n",
    "\n",
    "        # DataLoader mit Weighted Sampler\n",
    "        train_loader = DataLoader(\n",
    "            train_dataset,\n",
    "            batch_size=wandb.config.batch_size,\n",
    "            sampler=sampler,\n",
    "            drop_last=True \n",
    "        )\n",
    "    else:\n",
    "        # DataLoader ohne Weighted Sampling\n",
    "        train_loader = DataLoader(\n",
    "            train_dataset,\n",
    "            batch_size=wandb.config.batch_size,\n",
    "            shuffle=True,\n",
    "            drop_last=True \n",
    "        )\n",
    "\n",
    "    global val_loader\n",
    "    # Validation DataLoader (ohne Weighted Sampling, da nur für Evaluierung)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=wandb.config.batch_size, shuffle=False, drop_last=True) # Shuffle beim Validieren nicht notwendig\n",
    "\n",
    "\n",
    "\n",
    "def start_training():\n",
    "    # Überprüfen, ob eine GPU verfügbar ist\n",
    "    global device\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f'Using device: {device}')\n",
    "\n",
    "    # Wandb Initialisierung\n",
    "    wandb.init(\n",
    "        project=\"\",\n",
    "        config={\n",
    "            \"learning_rate\": 0.004252027091067649,\n",
    "            \"architecture\": \"MLP\",\n",
    "            \"dataset\": \"3boys\",\n",
    "            \"epochs\": 150,\n",
    "            \"batch_size\": 16,\n",
    "            \"optimizer\": \"AdamW\",  # Choices: 'AdamW', 'Adam', 'RMSprop'\n",
    "            \"weight_decay\": 0.0344653149484944,\n",
    "            \"loss_function\": \"BCE\",  # Choices: ['BCE', 'BCEWithLogits', 'Focal', 'Asymmetric', 'L', 'M']\n",
    "            \"loss_params\": {\n",
    "                \"focal_alpha\": 1,\n",
    "                \"focal_gamma\": 2,\n",
    "                \"asymmetric_gamma_neg\": 4,\n",
    "                \"asymmetric_gamma_pos\": 1,\n",
    "                \"asymmetric_clip\": 0.05,\n",
    "                \"l_beta\": 1,\n",
    "                \"m_beta\": 1\n",
    "            },\n",
    "            \"weighted_sampler\": True,\n",
    "            \"hidden_layers\": [64, 128, 64],\n",
    "            \"dropout_rate\": 0.14279874595232844,\n",
    "            \"early_stopping_patience\": 20,\n",
    "            \"beta_score\": 2.0\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Daten vorbereiten\n",
    "    prepare_data()\n",
    "\n",
    "    # Train model\n",
    "    num_features = X_train.shape[1]  \n",
    "    model = BinaryClassifier(input_features=num_features)\n",
    "    train_model(model, train_loader, val_loader)\n",
    "\n",
    "    # Final evaluation\n",
    "    model.load_state_dict(torch.load('best_model.pth'))\n",
    "    criterion = get_loss_function(wandb.config.loss_function)\n",
    "    model.eval()\n",
    "    evaluate(model, val_loader, criterion, is_final=True, is_test=False)\n",
    "    wandb.finish()\n",
    "\n",
    "\n",
    "start_training()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Sweep GPU-Basismodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sweep_config = {\n",
    "    'method': 'bayes', \n",
    "    'metric': {\n",
    "        'name': 'val_f2',\n",
    "        'goal': 'maximize'\n",
    "    },\n",
    "    'parameters': {\n",
    "        'learning_rate': {\n",
    "            'min': 0.0001,\n",
    "            'max': 0.01\n",
    "        },\n",
    "        'batch_size': {\n",
    "            'values': [8, 16, 32, 64]\n",
    "        },\n",
    "        'optimizer': {\n",
    "            'values': ['AdamW', 'Adam', 'RMSprop']\n",
    "        },\n",
    "        'weight_decay': {\n",
    "            'min': 0.001,\n",
    "            'max': 0.1\n",
    "        },\n",
    "        'loss_function': {\n",
    "            'values': ['BCE', 'BCEWithLogits'] # 'Focal', 'Asymmetric', 'L', 'M'\n",
    "        },\n",
    "        'weighted_sampler': {\n",
    "            'values': [True, False]  \n",
    "        },\n",
    "        'hidden_layers': {\n",
    "            'values': [[16, 8], [8, 16], [32, 16], [64, 32], [8, 4, 2], [16, 8, 4], [32, 16, 8], [64, 32, 16], [128, 64, 32], [256, 128, 64], [64, 128, 64], [32, 64, 32], [16, 32, 16], [8, 16, 8], [4, 8, 8, 4], [4, 8, 16, 8], [8, 16, 16, 8], [8, 16, 32, 16, 8]]\n",
    "        },\n",
    "        'dropout_rate': {\n",
    "            'min': 0.1,\n",
    "            'max': 0.8\n",
    "        },\n",
    "        'epochs': {\n",
    "            'value': 150 \n",
    "        },\n",
    "        'early_stopping_patience': {\n",
    "            'value': 10\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "project_name = \"\"\n",
    "sweep_id = wandb.sweep(sweep_config, project=project_name)\n",
    "wandb.agent(sweep_id, function=start_training, count=150)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BinaryClassifier(input_features=5).to(device)\n",
    "model.load_state_dict(torch.load('model.pth', map_location=device))\n",
    "\n",
    "model.eval()\n",
    "\n",
    "evaluate(model, val_loader, is_final=True, is_test=True)\n",
    "\n",
    "predictions = []\n",
    "true_labels = []\n",
    "with torch.no_grad():\n",
    "    for X_batch, y_batch in val_loader:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        outputs = model(X_batch)\n",
    "        predictions.extend((torch.sigmoid(outputs) > 0.5).squeeze().cpu().numpy())\n",
    "        true_labels.extend(y_batch.cpu().numpy())\n",
    "\n",
    "print(\"\\nFinal Classification Report:\")\n",
    "print(classification_report(true_labels, predictions))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BinaryClassifier(input_features=5).to(device)\n",
    "model.load_state_dict(torch.load('best_model.pth', map_location=device))\n",
    "\n",
    "model.eval()\n",
    "\n",
    "def predict_with_bias(df, bias=0.0, batch_size=64):\n",
    "    predictions = []\n",
    "    \n",
    "    try:\n",
    "        features = df[['audio_complexity', 'word_complexity', 'sentence_complexity',\n",
    "               'word_importance', 'word_occurrence']].astype('float32').values\n",
    "        features = torch.tensor(features, dtype=torch.float32).to(device)\n",
    "\n",
    "        \n",
    "        # Batched predictions\n",
    "        with torch.no_grad():\n",
    "            for i in range(0, len(features), batch_size):\n",
    "                batch = features[i:i+batch_size]\n",
    "                \n",
    "                # Predictions\n",
    "                outputs = model(batch)\n",
    "                \n",
    "                # Add Bias \n",
    "                outputs += bias\n",
    "            \n",
    "                predictions.extend((torch.sigmoid(outputs) > 0.5).cpu().numpy().flatten())\n",
    "        \n",
    "        return predictions\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Fehler bei der Vorhersage: {e}\")\n",
    "        return None\n",
    "\n",
    "df.loc[~df['set_manually'], 'display'] = predict_with_bias(df.loc[~df['set_manually']], bias=0.0)\n",
    "\n",
    "# More positive classifications:\n",
    "# df.loc[~df['set_manually'], 'display'] = predict_with_bias(dfa, bias=0.1)  # Apply positive bias for more display=True\n",
    "\n",
    "# More negative classifications:\n",
    "# df.loc[~df['set_manually'], 'display']= predict_with_bias(dfb, bias=-0.1)  # Apply negative bias for more display=False\n",
    "\n",
    "print(f\"Percentage displayed in df (no bias): {(df['display'].sum() / len(df)) * 100:.2f}%\")\n",
    "#print(f\"Percentage displayed in dfa (positive bias): {(df['display'].sum() / len(dfa)) * 100:.2f}%\")\n",
    "#print(f\"Percentage displayed in dfb (negative bias): {(df['display'].sum() / len(dfb)) * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(df.to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Subtitles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation_not_between_letters(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Removes punctuation marks (? , ; . -) only when they are not\n",
    "    between two alphanumeric characters.\n",
    "    \"\"\"\n",
    "    return re.sub(r'(?<![A-Za-z0-9])[?;.,=-]+|[?;.,=-]+(?![A-Za-z0-9])', '', text)\n",
    "\n",
    "def seconds_to_srt_timestamp(seconds: float) -> str:\n",
    "    \"\"\"\n",
    "    Converts time in seconds to SRT timestamp format \"HH:MM:SS,mmm\".\n",
    "    \"\"\"\n",
    "    if pd.isna(seconds):\n",
    "        return None\n",
    "    hours = int(seconds // 3600)\n",
    "    minutes = int((seconds % 3600) // 60)\n",
    "    secs = int(seconds % 60)\n",
    "    millis = int(round((seconds - int(seconds)) * 1000))\n",
    "    return f\"{hours:02}:{minutes:02}:{secs:02},{millis:03}\"\n",
    "\n",
    "def count_tokens(block_lines: list[str]) -> int:\n",
    "    \"\"\"\n",
    "    Counts number of alphanumeric tokens in block lines.\n",
    "    \"\"\"\n",
    "    count = 0\n",
    "    for line in block_lines[2:]:\n",
    "        words = line.split()\n",
    "        count += len([w for w in words if clean_token(w)])\n",
    "    return count\n",
    "\n",
    "def merge_and_clean_placeholders(words: list[str], placeholder: str) -> list[str]:\n",
    "    \"\"\"\n",
    "    Merges consecutive placeholders and removes placeholders\n",
    "    at start or end of sequence.\n",
    "    \"\"\"\n",
    "    merged_words = []\n",
    "    prev_w = None\n",
    "    for w in words:\n",
    "        # Avoid consecutive placeholders\n",
    "        if w == placeholder and prev_w == placeholder:\n",
    "            continue\n",
    "        merged_words.append(w)\n",
    "        prev_w = w\n",
    "    # Remove leading/trailing placeholders\n",
    "    while merged_words and merged_words[0] == placeholder:\n",
    "        merged_words.pop(0)\n",
    "    while merged_words and merged_words[-1] == placeholder:\n",
    "        merged_words.pop()\n",
    "    return merged_words\n",
    "\n",
    "def create_srt_file(\n",
    "    srt_lines: list[str],\n",
    "    df: pd.DataFrame,\n",
    "    new_srt_file: str,\n",
    "    original_timesteps: bool = False,\n",
    "    languages: list[str] = [\"en\", \"de\"]\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Creates new SRT file with following parameters:\n",
    "    - original_timesteps: If True, keeps original timestamps;\n",
    "                         otherwise shifts start time to first displayed word\n",
    "    - languages: List with \"en\" (original) and/or \"de\" (translation).\n",
    "                 If both included, writes two blocks.\n",
    "    \"\"\"\n",
    "    placeholder = '•'\n",
    "    display_flags = df[\"display\"].tolist()\n",
    "    df_index = 0\n",
    "\n",
    "    with open(new_srt_file, 'w', encoding='utf-8') as outfile:\n",
    "        subtitle_block = []\n",
    "        for line in srt_lines:\n",
    "            # Block ends with empty line\n",
    "            if line.strip() == '':\n",
    "                if subtitle_block:\n",
    "                    _write_block(subtitle_block, display_flags, df_index, df,\n",
    "                               outfile, placeholder, languages, original_timesteps)\n",
    "                    df_index += count_tokens(subtitle_block)\n",
    "                    subtitle_block = []\n",
    "                outfile.write('\\n')\n",
    "            else:\n",
    "                subtitle_block.append(line)\n",
    "\n",
    "        # Process final block\n",
    "        if subtitle_block:\n",
    "            _write_block(subtitle_block, display_flags, df_index, df,\n",
    "                        outfile, placeholder, languages, original_timesteps)\n",
    "\n",
    "def _write_block(block_lines: list[str],\n",
    "                display_flags: list[bool],\n",
    "                start_idx: int,\n",
    "                df: pd.DataFrame,\n",
    "                outfile,\n",
    "                placeholder: str,\n",
    "                languages: list[str],\n",
    "                original_timesteps: bool):\n",
    "    \"\"\"\n",
    "    Writes according to languages:\n",
    "      - original only (en)\n",
    "      - translation only (de)\n",
    "      - both (en + de)\n",
    "\n",
    "    Timestamp either kept (original_timesteps=True)\n",
    "    or set to first displayed word (original_timesteps=False)\n",
    "    \"\"\"\n",
    "    if len(block_lines) < 2:\n",
    "        return\n",
    "\n",
    "    # 1) Nummer und Timestamp extrahieren\n",
    "    subtitle_number = block_lines[0].strip()\n",
    "    timestamp_line = block_lines[1].strip()\n",
    "    text_lines = block_lines[2:]\n",
    "    token_count = count_tokens(block_lines)\n",
    "\n",
    "    # 2) Erstes angezeigtes Wort finden (falls original_timesteps=False)\n",
    "    first_displayed_idx = None\n",
    "    for i in range(token_count):\n",
    "        current_idx = start_idx + i\n",
    "        if current_idx < len(display_flags) and display_flags[current_idx]:\n",
    "            first_displayed_idx = current_idx\n",
    "            break\n",
    "\n",
    "    # Timestamp anpassen, falls gewünscht\n",
    "    try:\n",
    "        original_start, original_end = timestamp_line.split(' --> ')\n",
    "    except ValueError:\n",
    "        print(f\"Warnung: Ungültiges Timestamp-Format in Block {subtitle_number}.\")\n",
    "        original_start, original_end = \"00:00:00,000\", \"00:00:00,000\"\n",
    "\n",
    "    if not original_timesteps and first_displayed_idx is not None:\n",
    "        timestep = df.loc[first_displayed_idx, 'timestep']\n",
    "        if not pd.isna(timestep):\n",
    "            new_start_time = seconds_to_srt_timestamp(timestep)\n",
    "            if new_start_time:\n",
    "                timestamp_line = f\"{new_start_time} --> {original_end}\"\n",
    "\n",
    "    # 3) Tokens sammeln und entscheiden, ob \"•\" eingefügt wird\n",
    "    #    (wenn zwischen zwei angezeigten Wörtern min. eines übersprungen wurde)\n",
    "    filtered_tokens_per_lang = {lang: [] for lang in languages}\n",
    "    df_index = start_idx\n",
    "    # Hier merken wir uns für jede Sprache,\n",
    "    # ob wir zuletzt \"Skip\" hatten (mindestens ein Wort ausgelassen).\n",
    "    skip_occurred = {lang: False for lang in languages}\n",
    "\n",
    "    for line in text_lines:\n",
    "        words = line.split()\n",
    "        for w in words:\n",
    "            t_clean = clean_token(w)\n",
    "            # Prüfung: ist das ein echtes Wort oder nur z.B. Satzzeichen?\n",
    "            if t_clean is None:\n",
    "                # Wenn kein alphanumerisches Token -> wir ignorieren es hier,\n",
    "                # oder fügst du ggf. stattdessen placeholder hinzu\n",
    "                for lang in languages:\n",
    "                    # Wenn du Satzzeichen unbedingt behalten willst, kann man\n",
    "                    # line_parts[lang].append(w) tun. Sonst:\n",
    "                    pass\n",
    "                continue\n",
    "\n",
    "            # Jetzt prüfen, ob das Wort angezeigt wird\n",
    "            if df_index < len(display_flags) and display_flags[df_index]:\n",
    "                # Wenn vorher ein Skip war -> Platzhalter einfügen\n",
    "                for lang in languages:\n",
    "                    if skip_occurred[lang]:\n",
    "                        filtered_tokens_per_lang[lang].append(placeholder)\n",
    "                        skip_occurred[lang] = False\n",
    "                # Wort übernehmen\n",
    "                if \"en\" in languages:\n",
    "                    filtered_tokens_per_lang[\"en\"].append(w)\n",
    "                if \"de\" in languages:\n",
    "                    filtered_tokens_per_lang[\"de\"].append(df.loc[df_index, \"translation\"])\n",
    "            else:\n",
    "                # Wort wird ausgelassen -> Skip notieren\n",
    "                for lang in languages:\n",
    "                    skip_occurred[lang] = True\n",
    "            df_index += 1\n",
    "\n",
    "    # 4) Jetzt entfernen wir z.B. doppelte Platzhalter,\n",
    "    #    und bereinigen Satzzeichen, falls nötig\n",
    "    for lang in languages:\n",
    "        merged = merge_and_clean_placeholders(filtered_tokens_per_lang[lang], placeholder)\n",
    "        # Satzzeichen bereinigen\n",
    "        merged_line = remove_punctuation_not_between_letters(\" \".join(merged))\n",
    "\n",
    "        # Am Ende in eine Liste packen, damit wir pro Sprache final ausgeben können\n",
    "        filtered_tokens_per_lang[lang] = [merged_line] if merged_line else []\n",
    "\n",
    "    # 5) Finale Ausgabe: Jede Sprache wird zu genau einer Zeile zusammengefasst\n",
    "    for lang in languages:\n",
    "        text_lines_lang = filtered_tokens_per_lang[lang]\n",
    "        if text_lines_lang:\n",
    "            single_line = \" \".join(text_lines_lang)\n",
    "            outfile.write(f\"{subtitle_number}\\n\")\n",
    "            outfile.write(f\"{timestamp_line}\\n\")\n",
    "            outfile.write(f\"{single_line}\\n\\n\")\n",
    "\n",
    "\n",
    "# Extract display flags and create filtered SRT\n",
    "display_flags = df['display'].tolist()\n",
    "\n",
    "create_srt_file(\n",
    "    srt_lines=srt_lines_in_memory,\n",
    "    df=df,\n",
    "    new_srt_file=FILTERED_SRT,\n",
    "    original_timesteps=False,\n",
    "    languages=[\"en\", \"de\"]\n",
    ")\n",
    "\n",
    "print(f\"Filtered SRT written to: {FILTERED_SRT}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
